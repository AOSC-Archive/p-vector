#!/bin/python3
import os
import pathlib
import re
import signal
import subprocess
import sys
import threading
from datetime import timedelta, timezone

import pymongo
from pymongo.collection import Collection
from pymongo.cursor import Cursor
from pymongo.database import Database
from pymongo.errors import DuplicateKeyError

sys.path.insert(0, os.path.normpath(os.path.dirname(__file__) + '/../libexec/p-vector'))
from print import *
import pkgscan
import dpkg_version

interrupted = False

base_dir = '.'
release_info = ''

DATETIME_FORMAT = '%a, %d %b %Y %H:%M:%S %z'

def split_soname(s: str):
    r = re.compile('\.so(?!=[$.])')
    pos = r.search(s)
    if pos is None:
        return {'name': s, 'ver': ''}
    return {'name': s[:pos.end()], 'ver': s[pos.end():]}


def doc_from_pkg_scan(p):
    import bson
    pkg_doc = {
        'pkg': {
            'name': p.control['Package'],
            'ver': p.control['Version'],
            'comp_ver': dpkg_version.comparable_ver(p.control['Version']),
            'arch': p.control['Architecture'],
        },
        'deb': {
            'path': p.filename,
            'size': bson.int64.Int64(p.p.size),
            'hash': p.p.hash_value,
            'mtime': p.mtime
        },
        'time': p.p.time,
        'control': p.control,
        'relation': p.control.relations,
        'so_provides': [split_soname(i) for i in p.p.so_provides],
        'so_depends': [split_soname(i) for i in p.p.so_depends],
    }
    file_doc = []
    for f in p.p.files:
        doc = {
            'deb': p.p.hash_value,
            'pkg': pkg_doc['pkg'],
            'path': f.path,
            'is_dir': f.is_dir,
            'size': bson.int64.Int64(f.size),
            'type': f.type,
            'perm': f.perm,
            'uid': f.uid,
            'gid': f.gid,
            'uname': f.uname,
            'gname': f.gname,
        }
        doc['path'] = os.path.normpath(os.path.join('/', doc['path']))
        doc['base'] = os.path.basename(doc['path'])
        file_doc.append(doc)
    return pkg_doc, file_doc


def prune(pkg_col: Collection, pkg_old_col: Collection, file_col: Collection):
    delete_list = []
    delete_old_list = []

    cur = pkg_col.find()
    total = cur.count()
    count = 0
    for i in cur:
        if not pathlib.PosixPath(base_dir).joinpath(i['deb']['path']).exists():
            delete_list.append((i['deb']['path'], i['pkg']))
        count += 1
        progress_bar('Check current', count / total)

    cur = pkg_old_col.find()
    total = cur.count()
    count = 0
    for i in cur:
        if not pathlib.PosixPath(base_dir).joinpath(i['deb']['path']).exists():
            delete_old_list.append(i['deb']['path'])
        count += 1
        progress_bar('Check archive', count / total)

    total = len(delete_list)
    count = 0
    for i in delete_list:
        I('CLEAN', 'CUR   ', i[0])
        pkg_col.delete_many({'deb.path': i[0]})
        file_col.delete_many({'pkg': i[1]})
        count += 1
        progress_bar('Prune current', count / total)

    total = len(delete_old_list)
    count = 0
    for i in delete_old_list:
        I('CLEAN', 'OLD   ', i)
        pkg_old_col.delete_many({'deb.path': i})
        count += 1
        progress_bar('Prune archive', count / total)

    progress_bar_end('Prune database')
    return


def scan(pkg_col: Collection, pkg_old_col: Collection, file_col: Collection):
    from concurrent.futures import ThreadPoolExecutor
    executor = ThreadPoolExecutor(max_workers=os.cpu_count() + 10)
    futures = []
    counter_lock = threading.Lock()
    count = 0
    total = 0

    def signal_handler(sig, frame):
        global interrupted
        interrupted = True
        for future in futures:
            future.cancel()
        W('SCAN', 'Received SIGINT. Cancelled pending jobs.', file=sys.stderr)

    signal.signal(signal.SIGINT, signal_handler)

    def scan_deb(rel_path: str, mtime: int, status: str):
        deb_path = pathlib.PosixPath(base_dir).joinpath(rel_path)
        # Scan it.
        from subprocess import CalledProcessError
        try:
            p = pkgscan.scan(str(deb_path))
        except CalledProcessError as e:
            if e.returncode < 0 and signal.Signals(-e.returncode) == signal.SIGINT:
                W('SCAN', 'INTR  ', rel_path, file=sys.stderr)
                return
            if e.returncode in [1, 2]:
                E('SCAN', 'ERROR ', rel_path, 'is corrupted, status:', e.returncode)
                return
            raise
        if interrupted:
            return
        # Make a new document
        p.filename = rel_path
        p.mtime = mtime
        pkg_doc, file_doc = doc_from_pkg_scan(p)

        if status == 'update':
            # if a document points to the same path exists, replace it
            pkg_col.replace_one({'deb.path': rel_path}, pkg_doc)
            file_col.delete_many({'deb': p.p.hash_value})
            file_col.insert_many(file_doc)
            I('SCAN', 'UPDATE', rel_path)
        else:
            # if this is a new document
            try:
                old_pkg = pkg_col.find_one_and_replace(
                    {'pkg.name': pkg_doc['pkg']['name'],
                     'pkg.arch': pkg_doc['pkg']['arch'],
                     'pkg.comp_ver': {'$lt': pkg_doc['pkg']['comp_ver']}
                     }, pkg_doc, {'_id': False}, upsert=True)
            except DuplicateKeyError:
                same_ver_pkg = pkg_col.find_one({'pkg': pkg_doc['pkg']})
                if same_ver_pkg is None:
                    # This one is older, we do nothing. We had better delete it.
                    W('SCAN', 'OLD   ', pkg_doc['pkg']['arch'], pkg_doc['pkg']['name'], pkg_doc['pkg']['ver'])
                    pkg_old_col.insert_one(pkg_doc)
                else:
                    E('SCAN', 'DUP   ', rel_path, '==', same_ver_pkg['deb']['path'])
                return
            if old_pkg is not None:
                # This one is newer, put the old one into archive, insert this one.
                I('SCAN', 'NEWER ', pkg_doc['pkg']['arch'], pkg_doc['pkg']['name'],
                  pkg_doc['pkg']['ver'], '>>', old_pkg['pkg']['ver'])
                pkg_old_col.insert_one(old_pkg)
                file_col.delete_many({'pkg': old_pkg['pkg']})
                file_col.insert_many(file_doc)
            else:
                # Completely new package
                file_col.insert_many(file_doc)
                I('SCAN', 'NEW   ', pkg_doc['pkg']['arch'], pkg_doc['pkg']['name'], pkg_doc['pkg']['ver'])

    def containment_shell(*args):
        nonlocal count
        try:
            scan_deb(*args)
        except Exception as e:
            E('SCAN', e, 'with', args, file=sys.stderr)
            raise
        finally:
            with counter_lock:
                count += 1
            progress_bar('Scan .deb', count / total)

    deb_files = {}
    search_path = pathlib.PosixPath(base_dir)
    for i in search_path.rglob('*.deb'):
        if not i.is_file():
            continue
        s = i.stat()
        deb_files[str(i.relative_to(base_dir))] = {
            'size': s.st_size,
            'mtime': s.st_mtime_ns,
            'status': 'new'
        }

    def filter_deb_files(pkg_in_db: Cursor, files: dict):
        for cur in pkg_in_db:
            '''
            Suppose there are no changes if it has:
            * the same path (unique);
            * the same size;
            * the same modify time.
            We will not try to calculate hash now because it is too costly.
            '''
            path = cur['deb']['path']
            mtime = cur['deb']['mtime']
            size = cur['deb']['size']
            if files[path]['mtime'] != mtime or files[path]['size'] != size:
                files[path]['status'] = 'update'
            else:
                del files[path]

    query_exist = {'deb.path': {'$in': [path for path in deb_files]}}
    project_exist = {'_id': False, 'deb': True}
    filter_deb_files(pkg_old_col.find(query_exist, project_exist), deb_files)
    filter_deb_files(pkg_col.find(query_exist, project_exist), deb_files)

    total = len(deb_files)
    with executor:
        for path in deb_files:
            futures.append(executor.submit(
                containment_shell, path, deb_files[path]['mtime'], deb_files[path]['status']))
    if not interrupted:
        progress_bar_end('Scan .deb')
    else:
        print()


def check_file_dup(file_col: Collection):
    cur = file_col.aggregate([

        {'$match': {'is_dir': False}},  # Exclude directories

        {'$group': {  # Count packages that own the same file
            '_id': '$path',
            'count': {'$sum': 1},
            'pkgs': {'$push': {
                'name': '$pkg.name',
                'ver': '$pkg.ver'
            }}
        }},

        {'$match': {'count': {'$gt': 1}}},  # Pick the files which are contained in >1 packages

        {'$group': {  # Pick the first file as sample for each package set
            '_id': '$pkgs',
            'count': {'$sum': 1},
            'sample': {'$first': '$_id'}
        }},

    ], allowDiskUse=True)
    for i in cur:
        print(*[p['name'] + '@' + p['ver'] for p in i['_id']], end=': ')
        print(i['sample'], end='')
        if i['count'] == 1:
            print()
        else:
            print(' and', i['count'], 'more')


def gen_packages(pkg_col: Collection):
    cur = pkg_col.find({}, {'_id': 0, 'pkg': 1, 'deb': 1, 'control': 1}).sort([('pkg', pymongo.ASCENDING)])

    with open(base_dir + '/Packages', 'w', encoding='UTF-8') as f:
        count = 0
        total = cur.count()
        for i in cur:
            i['control']['Filename'] = i['deb']['path']
            i['control']['Size'] = str(i['deb']['size'])
            from binascii import hexlify
            i['control']['SHA256'] = hexlify(i['deb']['hash'])
            import deb822
            print(deb822.Deb822(i['control']), file=f)
            count += 1
            if count % 100 == 0:
                progress_bar('Generate Packages', count / total)
        progress_bar_end('Generate Packages')
    print('Compress Packages...')
    subprocess.check_call(['xz', '-k', '-0', '-f', base_dir + '/Packages'])


def gen_contents(pkg_col: Collection, file_col: Collection):
    cur = file_col.find({'is_dir': False}, {'_id': 0, 'pkg.name': 1, 'path': 1}).sort('path')
    arch = pkg_col.find_one({}, {'pkg.arch': 1})['pkg']['arch']
    with open(base_dir + '/Contents-' + arch, 'w', encoding='UTF-8') as f:
        count = 0
        total = cur.count()
        for i in cur:
            path = i['path'][1:]
            print(path, i['pkg']['name'], file=f)
            count += 1
            if count % 50000 == 0:
                progress_bar('Generate Contents', count / total)
        progress_bar_end('Generate Contents')
    print('Compress Contents...')
    subprocess.check_call(['xz', '-k', '-0', '-f', base_dir + '/Contents-' + arch])


def gen_release(pkg_col: Collection):
    arch = pkg_col.find_one({}, {'pkg.arch': 1})['pkg']['arch']
    import deb822
    r = deb822.Release(release_info)

    if 'Architectures' not in r:
        r['Architectures'] = arch

    def sha256_file(path: str):
        import Crypto.Hash.SHA256
        result = Crypto.Hash.SHA256.SHA256Hash()
        file_size = 0
        with open(base_dir + '/' + path, 'rb') as file:
            while True:
                block = file.read(8192)
                if len(block) == 0:
                    break
                result.update(block)
                file_size += len(block)
        return {
            'sha256': result.hexdigest(),
            'size': file_size,
            'name': path
        }

    now = datetime.now(tz=timezone.utc)

    r['Date'] = now.strftime(DATETIME_FORMAT)
    r['Valid-Until'] = (now + timedelta(weeks=2)).strftime(DATETIME_FORMAT)

    r['SHA256'] = [
        sha256_file('Contents-' + arch),
        sha256_file('Contents-' + arch + '.xz'),
        sha256_file('Packages'),
        sha256_file('Packages.xz'),
    ]

    print('Generate Release...')
    with open(base_dir + '/Release', 'w', encoding='UTF-8') as f:
        print(r, file=f)

    print('Sign Release...')
    subprocess.check_call(
        ['gpg', '--batch', '--yes', '--clearsign', '-o', base_dir + '/InRelease', base_dir + '/Release'])

    print('Clean Release...')
    os.remove(base_dir + '/Release')


def get_collections(db: Database, branch: str, component: str) -> (Collection, Collection, Collection):
    col_name = '%s/%s' % (branch, component)
    pkg_col = db[col_name]
    pkg_col.create_index([('deb.hash', pymongo.ASCENDING)], unique=True)
    pkg_col.create_index([('deb.path', pymongo.ASCENDING)], unique=True)
    pkg_col.create_index([('pkg', pymongo.ASCENDING)], unique=True)
    pkg_col.create_index([
        ('pkg.name', pymongo.ASCENDING),
        ('pkg.arch', pymongo.ASCENDING)],
        name='pkg_one_version', unique=True)
    pkg_col.create_index([('pkg.arch', pymongo.ASCENDING)])

    pkg_old_col = db[col_name + '.old']
    pkg_old_col.create_index([('deb.hash', pymongo.ASCENDING)], unique=True)
    pkg_old_col.create_index([('pkg', pymongo.ASCENDING)], unique=True)
    pkg_old_col.create_index([('pkg.arch', pymongo.ASCENDING)])

    file_col = db[col_name + '.files']
    file_col.create_index([('path', pymongo.ASCENDING)])
    file_col.create_index([('pkg', pymongo.ASCENDING)])
    file_col.create_index([('pkg.arch', pymongo.ASCENDING)])

    return pkg_col, pkg_old_col, file_col


def main():
    origin = sys.argv[1]
    with open(origin + '.db', 'r') as db_conn_str:
        client = pymongo.MongoClient(
            db_conn_str.readline().strip(),
            appname='p-vector-scanner', ssl_ca_certs=origin + '.crt')

    import yaml
    with open(origin + '.yml', 'rb') as f:
        y = yaml.load_all(f)
        global base_dir, release_info
        first_doc = next(y)
        base_dir = first_doc['path']
        del first_doc['path']
        #        OVERRIDE_RELEASE = y[0]
        db = client['p-vector']
        for i in y:
            if i is None:
                continue
            branch_name = i['branch']
            print('====', branch_name, '====')
            for component_dir in pathlib.PosixPath(base_dir).joinpath('pool').joinpath(branch_name).iterdir():
                if not component_dir.is_dir():
                    continue
                component_name = component_dir.name
                pkg_col, pkg_old_col, file_col = get_collections(db, branch_name, component_name)
                print(branch_name, component_name)
                # import check_elf
                # check_elf.check(pkg_col)
                # exit()
                scan(pkg_col, pkg_old_col, file_col)
                if interrupted:
                    exit(1)
                # prune(pkg_col, pkg_old_col, file_col)
                # gen_packages(pkg_col)
                # gen_contents(pkg_col, file_col)
                # gen_release(pkg_col)

    # check_elf_dep(pkg_col)


if __name__ == '__main__':
    main()
