#!/bin/python3
import os
import pathlib
import re
import subprocess
import sys
import threading
from datetime import timedelta, timezone

import pymongo
from pymongo.collection import Collection

sys.path.insert(0, os.path.normpath(os.path.dirname(__file__) + '/../libexec/p-vector'))
from print import *
import pkg_scan
import dpkg_version

REPO_DIR = '.'

DATETIME_FORMAT = '%a, %d %b %Y %H:%M:%S %z'

OVERRIDE_RELEASE = '''Origin: AOSC
Label: AOSC OS (amd64)
Suite: os-amd64
Codename: eMMC
Description: AOSC OS Repository (amd64)
'''


def split_soname(s: str):
    r = re.compile('\.so(?!=[$.])')
    pos = r.search(s)
    if pos is None:
        return {'name': s, 'ver': ''}
    return {'name': s[:pos.end()], 'ver': s[pos.end():]}


def doc_from_pkg_scan(p):
    pkg_doc = {
        'pkg': {
            'name': p.control['Package'],
            'ver': p.control['Version'],
            'arch': p.control['Architecture'],
        },
        'deb': {
            'path': p.filename,
            'size': p.p.size,
            'hash': p.p.hash_value,
            'mtime': p.mtime_ns
        },
        'time': p.p.time,
        'control': p.control,
        'relation': p.control.relations,
        'so_provides': [split_soname(i) for i in p.p.so_provides],
        'so_depends': [split_soname(i) for i in p.p.so_depends],
    }

    file_doc = []
    for f in p.p.files:
        doc = {
            'deb': p.p.hash_value,
            'pkg': pkg_doc['pkg'],
            'path': f.path,
            'is_dir': f.is_dir,
            'size': f.size,
            'type': f.type,
            'perm': f.perm,
            'uid': f.uid,
            'gid': f.gid,
            'uname': f.uname,
            'gname': f.gname,
        }
        doc['path'] = os.path.normpath(os.path.join('/', doc['path']))
        doc['base'] = os.path.basename(doc['path'])
        file_doc.append(doc)
    return pkg_doc, file_doc


def prune(pkg_col: Collection, pkg_old_col: Collection, file_col: Collection):
    delete_list = []
    delete_old_list = []

    cur = pkg_col.find()
    total = cur.count()
    count = 0
    for i in cur:
        if not pathlib.PosixPath(REPO_DIR).joinpath(i['deb']['path']).exists():
            delete_list.append((i['deb']['path'], i['pkg']))
        count += 1
        progress_bar('Check current', count / total)

    cur = pkg_old_col.find()
    total = cur.count()
    count = 0
    for i in cur:
        if not pathlib.PosixPath(REPO_DIR).joinpath(i['deb']['path']).exists():
            delete_old_list.append(i['deb']['path'])
        count += 1
        progress_bar('Check archive', count / total)

    total = len(delete_list)
    count = 0
    for i in delete_list:
        I('CLEAN', 'CUR   ', i[0])
        pkg_col.delete_many({'deb.path': i[0]})
        file_col.delete_many({'pkg': i[1]})
        count += 1
        progress_bar('Prune current', count / total)

    total = len(delete_old_list)
    count = 0
    for i in delete_old_list:
        I('CLEAN', 'OLD   ', i)
        pkg_old_col.delete_many({'deb.path': i})
        count += 1
        progress_bar('Prune archive', count / total)

    progress_bar_end('Prune database')
    return


def scan(pkg_col: Collection, pkg_old_col: Collection, file_col: Collection):
    free_slots = threading.Semaphore(os.cpu_count() + 2)
    db_lock = threading.Lock()
    counter_lock = threading.Lock()
    count = 0
    total = 0

    def scan_deb(deb_path):
        st = os.stat(deb_path)
        mtime_ns = st.st_mtime_ns
        size = st.st_size

        with db_lock:
            same_file = pkg_col.find_one({'deb.path': deb_path}, {'_id': 0})
            same_old_file = pkg_old_col.find_one({'deb.path': deb_path}, {'_id': 0})

        '''
        Suppose there are no changes if it has:
        * the same path (unique);
        * the same size;
        * the same modify time.
        
        We will not try to calculate hash now because it is too costly.
        '''
        if same_file is not None and \
                same_file['deb']['size'] == size and \
                same_file['deb']['mtime'] == mtime_ns:
            free_slots.release()
            return
        if same_old_file is not None and \
                same_old_file['deb']['size'] == size and \
                same_old_file['deb']['mtime'] == mtime_ns:
            free_slots.release()
            return

        # Scan it.
        from subprocess import CalledProcessError
        try:
            p = pkg_scan.scan(deb_path)
        except CalledProcessError as e:
            if e.returncode not in [1, 2]:
                raise
            E('SCAN', 'ERROR ', deb_path, 'is corrupted')
            return
        finally:
            free_slots.release()

        with db_lock:
            # Return if the same hash exists
            same_hash = pkg_old_col.find_one({'deb.hash': p.p.hash_value}, {'_id': 0})
            if same_hash is not None:
                E('SCAN', 'DUP   ', deb_path, '<=>', same_hash['deb']['path'])
                return
            same_hash = pkg_col.find_one({'deb.hash': p.p.hash_value}, {'_id': 0})
            if same_hash is not None:
                E('SCAN', 'DUP   ', deb_path, '<=>', same_hash['deb']['path'])
                return

            # Make a new document
            p.filename = deb_path
            p.mtime_ns = mtime_ns
            pkg_doc, file_doc = doc_from_pkg_scan(p)

            same_file = pkg_col.find_one({'deb.path': deb_path}, {'_id': 0})
            if same_file is not None:
                # if a document points to the same path exists, replace it
                pkg_col.replace_one({'deb.path': deb_path}, pkg_doc)
                file_col.delete_many({'deb': p.p.hash_value})
                file_col.insert_many(file_doc)
                I('SCAN', 'UPDATE', deb_path)
            else:
                # if this is a new document
                same_pkg = pkg_col.find_one({'pkg.name': pkg_doc['pkg']['name']}, {'_id': 0})
                if same_pkg is not None:
                    # if a package with the same name exists,
                    r = dpkg_version.compare_ver(same_pkg['pkg']['ver'], pkg_doc['pkg']['ver'])
                    if r > 0:
                        # This one is earlier, we do nothing. We had better delete it.
                        W('SCAN', 'OLD   ', deb_path, '<<', same_pkg['deb']['path'])
                        pkg_old_col.replace_one({'pkg': pkg_doc['pkg']}, pkg_doc, upsert=True)
                    if r == 0:
                        # They have the same version, it's just not right
                        # (They have different paths, different hashes, but same versions?
                        # Package Manager cannot decide to use which one).
                        E('SCAN', 'DUP   ', deb_path, '==', same_pkg['deb']['path'])
                    if r < 0:
                        # This one is newer, put the old one into archive, insert this one.
                        I('SCAN', 'NEWER ', deb_path, '>>', same_pkg['deb']['path'])
                        pkg_col.replace_one({'pkg.name': pkg_doc['pkg']['name']}, pkg_doc)
                        pkg_old_col.insert_one(same_pkg)
                        file_col.delete_many({'pkg': same_pkg['pkg']})
                        file_col.insert_many(file_doc)
                else:
                    # Completely new package
                    pkg_col.insert_one(pkg_doc)
                    file_col.insert_many(file_doc)
                    I('SCAN', 'NEW   ', deb_path)

    def containment_shell(deb_path):
        try:
            scan_deb(deb_path)
        except Exception:
            print('ERROR', deb_path, file=sys.stderr)
            raise
        finally:
            with counter_lock:
                nonlocal count
                count += 1
                progress_bar('Scan .deb', count / total)

    search_path = pathlib.PosixPath(REPO_DIR)
    jobs = list(search_path.rglob('*.deb'))
    jobs.sort()
    total = len(jobs)

    thread_pool = []
    for i in jobs:
        if i.is_file():
            free_slots.acquire()
            t = threading.Thread(target=containment_shell, args=[str(i)])
            thread_pool.append(t)
            t.start()
    for t in thread_pool:
        # Wait all threads
        t.join()

    progress_bar_end('Scan .deb')


def check_file_dup(file_col: Collection):
    cur = file_col.aggregate([

        {'$match': {'is_dir': False}},  # Exclude directories

        {'$group': {  # Count packages that own the same file
            '_id': '$path',
            'count': {'$sum': 1},
            'pkgs': {'$push': {
                'name': '$pkg.name',
                'ver': '$pkg.ver'
            }}
        }},

        {'$match': {'count': {'$gt': 1}}},  # Pick the files which are contained in >1 packages

        {'$group': {  # Pick the first file as sample for each package set
            '_id': '$pkgs',
            'count': {'$sum': 1},
            'sample': {'$first': '$_id'}
        }},

    ], allowDiskUse=True)
    for i in cur:
        print(*[p['name'] + '@' + p['ver'] for p in i['_id']], end=': ')
        print(i['sample'], end='')
        if i['count'] == 1:
            print()
        else:
            print(' and', i['count'], 'more')


def check_elf_dep(pkg_col: Collection):
    provides = pkg_col.aggregate([
        {'$project': {'so_provides': 1}},  # Optimise for RAM
        {'$unwind': '$so_provides'},
        {'$group': {'_id': '$so_provides'}},
    ])
    provides = [i['_id'] for i in provides]
    depends = pkg_col.aggregate([
        {'$project': {'so_depends': 1}},
        {'$unwind': '$so_depends'},
        {'$group': {'_id': '$so_depends'}},
    ])
    missing = []
    missing_name = []
    for _d in depends:
        d = _d['_id']
        found = False
        for p in provides:
            if p['name'] == d['name'] and (p['ver'] + '.').startswith(d['ver'] + '.'):
                found = True
                break
        if not found:
            missing.append(d)
            missing_name.append(d['name'])

    cur = pkg_col.aggregate([
        {'$match': {'so_depends': {'$in': missing}}},
        {'$project': {'_id': 0, 'pkg': 1, 'so_depends': 1}},
        {'$unwind': '$so_depends'},
        {'$match': {'so_depends': {'$in': missing}}},
        {'$lookup': {
            'from': 'pvector',
            'localField': 'so_depends.name',
            'foreignField': 'so_provides.name',
            'as': 'probably'
        }},
        {'$project': {'pkg': 1, 'so_depends': 1, 'probably.pkg': 1, 'probably.so_provides': 1}},
        {'$unwind': '$probably'},
        {'$unwind': '$probably.so_provides'},
        {'$match': {'probably.so_provides.name': {'$in': missing_name}}},
        {'$group': {
            '_id': {'$concat': ['$pkg.name', '(', '$pkg.ver', ')']},
            'broken_dep': {'$addToSet': '$so_depends'},
            'probably': {
                '$addToSet': {
                    'pkg': {'$concat': ['$probably.pkg.name', '(', '$probably.pkg.ver', ')']},
                    'so_provides': '$probably.so_provides'
                }
            },
        }},
    ])

    c = [i for i in cur]
    print(len(c))
    c.sort(key=lambda x: x['_id'])
    for i in c:
        pkg = i['_id']
        broken = i['broken_dep']
        probably = {}
        for p in i['probably']:
            for d in broken:
                if p['so_provides']['name'] == d['name']:
                    if p['pkg'] not in probably:
                        probably[p['pkg']] = []
                    probably[p['pkg']].append(p['so_provides']['name'] + p['so_provides']['ver'])
                    break
        print(pkg + ':')

        broken = [b['name'] + b['ver'] for b in broken]
        broken.sort()
        print('  missing:', ', '.join(broken))

        if len(probably) != 0:
            print('  hints:')
            for p in probably:
                probably[p].sort()
                print('    - ' + p + ' has ' + ', '.join(probably[p]))
        print()


def gen_packages(pkg_col: Collection):
    cur = pkg_col.find({}, {'_id': 0, 'pkg': 1, 'deb': 1, 'control': 1}).sort([('pkg', pymongo.ASCENDING)])

    with open(REPO_DIR + '/Packages', 'w', encoding='UTF-8') as f:
        count = 0
        total = cur.count()
        for i in cur:
            i['control']['Filename'] = i['deb']['path']
            i['control']['Size'] = str(i['deb']['size'])
            from binascii import hexlify
            i['control']['SHA256'] = hexlify(i['deb']['hash'])
            import deb822
            print(deb822.Deb822(i['control']), file=f)
            count += 1
            if count % 100 == 0:
                progress_bar('Generate Packages', count / total)
        progress_bar_end('Generate Packages')
    print('Compress Packages...')
    subprocess.check_call(['xz', '-k', '-0', '-f', REPO_DIR + '/Packages'])


def gen_contents(pkg_col: Collection, file_col: Collection):
    cur = file_col.find({'is_dir': False}, {'_id': 0, 'pkg.name': 1, 'path': 1}).sort('path')
    arch = pkg_col.find_one({}, {'pkg.arch': 1})['pkg']['arch']
    with open(REPO_DIR + '/Contents-' + arch, 'w', encoding='UTF-8') as f:
        count = 0
        total = cur.count()
        for i in cur:
            path = i['path'][1:]
            print(path, i['pkg']['name'], file=f)
            count += 1
            if count % 50000 == 0:
                progress_bar('Generate Contents', count / total)
        progress_bar_end('Generate Contents')
    print('Compress Contents...')
    subprocess.check_call(['xz', '-k', '-0', '-f', REPO_DIR + '/Contents-' + arch])


def gen_release(pkg_col: Collection):
    arch = pkg_col.find_one({}, {'pkg.arch': 1})['pkg']['arch']
    import deb822
    r = deb822.Release(OVERRIDE_RELEASE)

    if 'Architectures' not in r:
        r['Architectures'] = arch

    def sha256_file(path: str):
        import Crypto.Hash.SHA256
        result = Crypto.Hash.SHA256.SHA256Hash()
        file_size = 0
        with open(REPO_DIR + '/' + path, 'rb') as file:
            while True:
                block = file.read(8192)
                if len(block) == 0:
                    break
                result.update(block)
                file_size += len(block)
        return {
            'sha256': result.hexdigest(),
            'size': file_size,
            'name': path
        }

    now = datetime.now(tz=timezone.utc)

    r['Date'] = now.strftime(DATETIME_FORMAT)
    r['Valid-Until'] = (now + timedelta(weeks=2)).strftime(DATETIME_FORMAT)

    r['SHA256'] = [
        sha256_file('Contents-' + arch),
        sha256_file('Contents-' + arch + '.xz'),
        sha256_file('Packages'),
        sha256_file('Packages.xz'),
    ]

    print('Generate Release...')
    with open(REPO_DIR + '/Release', 'w', encoding='UTF-8') as f:
        print(r, file=f)

    print('Sign Release...')
    subprocess.check_call(
        ['gpg', '--batch', '--yes', '--clearsign', '-o', REPO_DIR + '/InRelease', REPO_DIR + '/Release'])

    print('Clean Release...')
    os.remove(REPO_DIR + '/Release')


def main():
    client = pymongo.MongoClient('localhost', 27017)
    db = client['aosc-os']

    pkg_col = db['pvector']
    pkg_col.create_index([('deb.hash', pymongo.ASCENDING)], name='hash_unique', unique=True)
    pkg_col.create_index([('deb.path', pymongo.ASCENDING)], name='path_unique', unique=True)
    pkg_col.create_index([('pkg', pymongo.ASCENDING)], name='pkg_ascending', unique=True)

    pkg_old_col = db['pvector_old']
    pkg_old_col.create_index([('deb.hash', pymongo.ASCENDING)], name='hash_unique', unique=True)
    pkg_old_col.create_index([('pkg', pymongo.ASCENDING)], name='pkg_ascending', unique=True)

    file_col = db['pvector_files']
    file_col.create_index([('path', pymongo.ASCENDING)], name='path_ascending')
    file_col.create_index([('pkg', pymongo.ASCENDING)], name='pkg_ascending')

    scan(pkg_col, pkg_old_col, file_col)
    prune(pkg_col, pkg_old_col, file_col)
    gen_packages(pkg_col)
    gen_contents(pkg_col, file_col)
    gen_release(pkg_col)
    # check_elf_dep(pkg_col)


if __name__ == '__main__':
    main()
